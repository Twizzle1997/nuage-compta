{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('data_science': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4732c83932685b0664faa5679aa5d57be970d81d9b5c25b3f14d84dbd2e895e8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NUAGE COMPTA ET LA FACTURE S'ANALYSA\n",
    "# Création du modèle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 0. Import des librairies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.contours import sort_contours\n",
    "\n",
    "from src import fonctions"
   ]
  },
  {
   "source": [
    "## 0. Paramétrage de l'application"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins du projet\n",
    "DATA_ROOT = 'data/'\n",
    "MODELE_PATH = DATA_ROOT + 'model.h5'\n",
    "TRAINING_PATH = DATA_ROOT + 'dataset_reduit_train'\n",
    "TESTING_PATH = DATA_ROOT + 'dataset_reduit_test'\n",
    "\n",
    "# Autres paramètres\n",
    "batch_size = 32\n",
    "num_classes = 27\n",
    "epochs = 35\n",
    "img_size = 28\n",
    "input_shape = (img_size, img_size, 1)"
   ]
  },
  {
   "source": [
    "## 1. Optimisation et nettoyage des données"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Afin de réduire les temps d'attente et dans un soucis d'optimisation, je decide d'importer une fonction qui prend 200 png par lettres (suffisant ici). Ces images viennent s'ajouter au dossier dataset_reduit_train ###\n",
    "fonctions.import_fichier_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Afin de réduire les temps d'attente et dans un soucis d'optimisation, je decide d'importer une fonction qui prend 3 png par lettres (suffisant ici). Ces images viennent s'ajouter au dossier dataset_reduit_test ###\n",
    "fonctions.import_fichier_test()"
   ]
  },
  {
   "source": [
    "## 2. Import des Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 5207 images belonging to 27 classes.\nFound 1567 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "### ImageDataGenerator génère des lots de données d'image vectorielles, convertissant les coefficients RVB compris entre 0 et 255 en valeurs cibles comprises entre 0 et 1 par mise à l'échelle avec un facteur de 1/255 à l' aide de la remise à l' échelle ###\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    ### shear_range est utilisé pour appliquer de manière aléatoire des transformations de cisaillement ###\n",
    "    shear_range = 0.2,\n",
    "    ### zoom_range est utilisé pour zoomer aléatoirement à l'intérieur des images ###           \n",
    "    zoom_range = 0.2,\n",
    "    ### horizontal_flip est utilisé pour retourner au hasard la moitié des images horizontalement ###            \n",
    "    horizontal_flip = True\n",
    ")      \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "\n",
    "### J'importe les images une par une à partir des répertoires en utilisant .flow_from_directory et y appliquons ImageDataGenerator ###\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    ### Choix de mon repertoire ###\n",
    "    directory = TRAINING_PATH,\n",
    "    ### Je converti les images de leur taille d'origine à notre target_size ###                    \n",
    "    target_size = (img_size,img_size),\n",
    "    ### Nombre batch_size qui fait référence au nombre d'exemples d'entraînement utilisés dans une itération ###                                      \n",
    "    batch_size = batch_size,\n",
    "    ### Je definis le class_mode sur \"catégorical\" indiquant que nous avons plusieurs classes (a à z) à prédire ###          \n",
    "    class_mode = \"categorical\",\n",
    "    ### Je choisis le color_mode \"grayscale\", indiquant que nous trvaillons sur une image en noir et blanc\n",
    "    color_mode = \"grayscale\"                                \n",
    "\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = TESTING_PATH,\n",
    "    target_size = (img_size,img_size),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = \"categorical\",\n",
    "    color_mode = \"grayscale\"\n",
    "\n",
    ")"
   ]
  },
  {
   "source": [
    "## 3. Definition du modèle d'architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 11, 11, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 800)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               102528    \n_________________________________________________________________\ndense_2 (Dense)              (None, 27)                3483      \n=================================================================\nTotal params: 115,579\nTrainable params: 115,579\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Création d'un modèle séquentiel qui permet de définir l'architecture CNN couche par couche à l'aide de la fonction .add .Nous ajoutons d'abord une couche de convolution avec 32 filtres de taille 3X3 sur les images d'entrée et la passons à travers la fonction d'activation 'relu'.Nous effectuons ensuite des opérations MaxPooling en utilisant un pool de taille 2X2 ###\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape = input_shape, activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "### Ces couches sont ensuite répétées à nouveau pour améliorer les performances du modèle ###\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "### Enfin, nous aplatissons notre matrice résultante et la passons à travers une couche dense composée de 128 nœuds. Celui-ci est ensuite connecté à la couche de sortie constituée de 26 nœuds, chaque nœud représentant un alphabet ###\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128, activation = \"relu\"))\n",
    "model.add(Dense(units = 27, activation = \"softmax\"))            ### Activation softmax qui convertit les scores en une distribution de probabilité normalisée, et                                                                   le nœud avec la probabilité la plus élevée est sélectionné comme sortie ###\n",
    "\n",
    "### Une fois notre architecture CNN définie, nous compilons le modèle à l'aide de l'optimiseur Adam ###\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## 4. Entrainement et enregistrement du modèle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/35\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 3.1392 - accuracy: 0.1182 - val_loss: 2.7065 - val_accuracy: 0.2354\n",
      "Epoch 2/35\n",
      "32/32 [==============================] - 4s 135ms/step - loss: 2.2266 - accuracy: 0.3564 - val_loss: 1.6633 - val_accuracy: 0.4624\n",
      "Epoch 3/35\n",
      "32/32 [==============================] - 4s 112ms/step - loss: 1.6654 - accuracy: 0.4906 - val_loss: 1.5131 - val_accuracy: 0.5908\n",
      "Epoch 4/35\n",
      "32/32 [==============================] - 5s 142ms/step - loss: 1.3177 - accuracy: 0.6123 - val_loss: 1.3906 - val_accuracy: 0.6481\n",
      "Epoch 5/35\n",
      "32/32 [==============================] - 4s 137ms/step - loss: 1.1478 - accuracy: 0.6436 - val_loss: 1.4387 - val_accuracy: 0.7019\n",
      "Epoch 6/35\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.0564 - accuracy: 0.6689 - val_loss: 0.9646 - val_accuracy: 0.7285\n",
      "Epoch 7/35\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.9286 - accuracy: 0.7109 - val_loss: 0.6093 - val_accuracy: 0.7537\n",
      "Epoch 8/35\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.8726 - accuracy: 0.7330 - val_loss: 1.3828 - val_accuracy: 0.7576\n",
      "Epoch 9/35\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.8468 - accuracy: 0.7393 - val_loss: 0.3491 - val_accuracy: 0.7881\n",
      "Epoch 10/35\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.7738 - accuracy: 0.7490 - val_loss: 0.5939 - val_accuracy: 0.8025\n",
      "Epoch 11/35\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.6630 - accuracy: 0.7861 - val_loss: 0.6447 - val_accuracy: 0.8006\n",
      "Epoch 12/35\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.6570 - accuracy: 0.8008 - val_loss: 0.3848 - val_accuracy: 0.8242\n",
      "Epoch 13/35\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.6270 - accuracy: 0.7941 - val_loss: 0.4052 - val_accuracy: 0.8250\n",
      "Epoch 14/35\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.6125 - accuracy: 0.8193 - val_loss: 0.4216 - val_accuracy: 0.8113\n",
      "Epoch 15/35\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.5801 - accuracy: 0.8242 - val_loss: 0.2369 - val_accuracy: 0.8408\n",
      "Epoch 16/35\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.5703 - accuracy: 0.8193 - val_loss: 0.5670 - val_accuracy: 0.8397\n",
      "Epoch 17/35\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.5782 - accuracy: 0.8271 - val_loss: 0.4218 - val_accuracy: 0.8456\n",
      "Epoch 18/35\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.4675 - accuracy: 0.8650 - val_loss: 0.3443 - val_accuracy: 0.8701\n",
      "Epoch 19/35\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.5057 - accuracy: 0.8516 - val_loss: 0.3689 - val_accuracy: 0.8553\n",
      "Epoch 20/35\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.4905 - accuracy: 0.8564 - val_loss: 0.4277 - val_accuracy: 0.8543\n",
      "Epoch 21/35\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.4711 - accuracy: 0.8502 - val_loss: 0.2038 - val_accuracy: 0.8691\n",
      "Epoch 22/35\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.4099 - accuracy: 0.8711 - val_loss: 0.2753 - val_accuracy: 0.8710\n",
      "Epoch 23/35\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.4029 - accuracy: 0.8838 - val_loss: 0.5437 - val_accuracy: 0.8729\n",
      "Epoch 24/35\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.4495 - accuracy: 0.8555 - val_loss: 0.5663 - val_accuracy: 0.8740\n",
      "Epoch 25/35\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.4430 - accuracy: 0.8672 - val_loss: 0.3590 - val_accuracy: 0.8847\n",
      "Epoch 26/35\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.3668 - accuracy: 0.8936 - val_loss: 0.2144 - val_accuracy: 0.8877\n",
      "Epoch 27/35\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3609 - accuracy: 0.8926 - val_loss: 0.2854 - val_accuracy: 0.8837\n",
      "Epoch 28/35\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3736 - accuracy: 0.8828 - val_loss: 0.5473 - val_accuracy: 0.8925\n",
      "Epoch 29/35\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3802 - accuracy: 0.8828 - val_loss: 0.9824 - val_accuracy: 0.8877\n",
      "Epoch 30/35\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3813 - accuracy: 0.8828 - val_loss: 0.5010 - val_accuracy: 0.8788\n",
      "Epoch 31/35\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.3453 - accuracy: 0.8945 - val_loss: 0.3610 - val_accuracy: 0.8944\n",
      "Epoch 32/35\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3250 - accuracy: 0.9034 - val_loss: 0.1405 - val_accuracy: 0.8848\n",
      "Epoch 33/35\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3359 - accuracy: 0.8965 - val_loss: 0.1798 - val_accuracy: 0.8954\n",
      "Epoch 34/35\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3264 - accuracy: 0.8887 - val_loss: 0.3254 - val_accuracy: 0.9042\n",
      "Epoch 35/35\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3574 - accuracy: 0.8896 - val_loss: 0.1373 - val_accuracy: 0.9102\n",
      "Test de perte: 0.5439829230308533\n",
      "Test de précision: 0.9099289178848267\n",
      "Enregistrement du modèle...\n",
      "Modèle enregistré!\n"
     ]
    }
   ],
   "source": [
    "### Je décide de créer 25 répétitions, et j'ai X2 les steps_per_epoch pour augmenter notre précision ###\n",
    "\n",
    "entrainement = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = test_generator,\n",
    "    validation_steps = batch_size\n",
    ")\n",
    "\n",
    "score = model.evaluate(train_generator, verbose=0)\n",
    "print(\"Test de perte:\", score[0])\n",
    "print(\"Test de précision:\", score[1])\n",
    "print(\"Enregistrement du modèle...\")\n",
    "model.save(MODELE_PATH)\n",
    "print(\"Modèle enregistré!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}